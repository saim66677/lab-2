
import numpy as np
import matplotlib.pyplot as plt
X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([[0],[1],[1],[0]])
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def sigmoid_derivative(x):
    return x * (1 - x)

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)


def train_mlp(X, y, lr=0.1, epochs=1000, activation="sigmoid"):
    np.random.seed(42)
    w1 = np.random.randn(2,2)
    b1 = np.zeros((1,2))
    w2 = np.random.randn(2,1)
    b2 = np.zeros((1,1))


    act = sigmoid if activation=="sigmoid" else relu
    act_deriv = sigmoid_derivative if activation=="sigmoid" else relu_derivative

    losses = []
    for epoch in range(epochs):
    
        z1 = X.dot(w1) + b1
        a1 = act(z1)
        z2 = a1.dot(w2) + b2
        a2 = sigmoid(z2) 
         error = y - a2
        loss = np.mean(error**2)
        losses.append(loss)
    
        d_a2 = error * sigmoid_derivative(a2)
        d_w2 = a1.T.dot(d_a2)
        d_b2 = np.sum(d_a2, axis=0, keepdims=True)

        d_a1 = d_a2.dot(w2.T) * act_deriv(a1)
        d_w1 = X.T.dot(d_a1)
        d_b1 = np.sum(d_a1, axis=0, keepdims=True)
        w1 += lr * d_w1
        b1 += lr * d_b1
        w2 += lr * d_w2
        b2 += lr * d_b2

    return a2, a2.round(), losses

settings = [(0.01, 1000), (0.1, 5000), (0.5, 500)]
for lr, ep in settings:
    raw_preds, preds, losses = train_mlp(X, y, lr=lr, epochs=ep, activation="sigmoid")
    print(f"Learning Rate={lr}, Epochs={ep}")
    print("Predictions:", preds.T)
    print("Accuracy:", np.mean(preds==y), "\n")
    plt.plot(losses, label=f"lr={lr}, ep={ep}")

plt.legend()
plt.title("Loss Curves for Different Hyperparameters")
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.show()


def confusion_matrix(y_true, y_pred):
    TP = np.sum((y_true==1) & (y_pred==1))
    TN = np.sum((y_true==0) & (y_pred==0))
    FP = np.sum((y_true==0) & (y_pred==1))
    FN = np.sum((y_true==1) & (y_pred==0))
    return TP, TN, FP, FN

def accuracy_score(y_true, y_pred):
    return np.mean(y_true==y_pred)

def precision_score(y_true, y_pred):
    TP, TN, FP, FN = confusion_matrix(y_true, y_pred)
    return TP / (TP + FP + 1e-9)

def recall_score(y_true, y_pred):
    TP, TN, FP, FN = confusion_matrix(y_true, y_pred)
    return TP / (TP + FN + 1e-9)

def f1_score(y_true, y_pred):
    p = precision_score(y_true, y_pred)
    r = recall_score(y_true, y_pred)
    return 2*p*r / (p+r+1e-9)


raw_preds, preds, losses = train_mlp(X, y, lr=0.1, epochs=5000, activation="sigmoid")

print("=== Final Evaluation ===")
print("Accuracy :", accuracy_score(y, preds))
print("Precision:", precision_score(y, preds))
print("Recall   :", recall_score(y, preds))
print("F1 Score :", f1_score(y, preds))


def roc_curve(y_true, y_scores):
    thresholds = np.linspace(0,1,50)
    tpr_list, fpr_list = [], []

    for t in thresholds:
        y_pred = (y_scores >= t).astype(int)
        TP, TN, FP, FN = confusion_matrix(y_true, y_pred)
        tpr = TP / (TP + FN + 1e-9)   
        fpr = FP / (FP + TN + 1e-9)
        tpr_list.append(tpr)
        fpr_list.append(fpr)

    return fpr_list, tpr_list

fpr, tpr = roc_curve(y, raw_preds)
plt.plot(fpr, tpr, marker="o", label="MLP (XOR)")
plt.plot([0,1],[0,1],"k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate (Recall)")
plt.title("ROC Curve for XOR MLP")
plt.legend()
plt.show()
